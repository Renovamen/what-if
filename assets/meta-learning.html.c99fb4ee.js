import{d as e}from"./app.3e879cdf.js";import{_ as r}from"./plugin-vue_export-helper.21dcd24c.js";const a={},n=e('<h1 id="meta-learning-few-shot-learning" tabindex="-1"><a class="header-anchor" href="#meta-learning-few-shot-learning" aria-hidden="true">#</a> Meta Learning / Few-Shot Learning</h1><p>Literatures of Meta Learning (\u5143\u5B66\u4E60) / Few-Shot Learning (\u5C0F\u6837\u672C\u5B66\u4E60).</p><h2 id="other-awesome-lists" tabindex="-1"><a class="header-anchor" href="#other-awesome-lists" aria-hidden="true">#</a> Other Awesome Lists</h2><ul><li><a href="https://github.com/sudharsan13296/Awesome-Meta-Learning" target="_blank" rel="noopener noreferrer">sudharsan13296/Awesome-Meta-Learning</a></li></ul><h2 id="courses-tutorials" tabindex="-1"><a class="header-anchor" href="#courses-tutorials" aria-hidden="true">#</a> Courses &amp; Tutorials</h2><ul><li><p><strong>Meta Learning.</strong> <em>Hung-yi Lee (\u674E\u5B8F\u6BC5).</em> [Slides: <a href="https://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Meta1%20(v6).pdf" target="_blank" rel="noopener noreferrer">Part 1</a>, <a href="https://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Meta2%20(v4).pdf" target="_blank" rel="noopener noreferrer">Part 2</a>] <a href="https://www.youtube.com/watch?v=EkAqYbpCYAc&amp;list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&amp;index=33&amp;t=0s" target="_blank" rel="noopener noreferrer">[Video]</a></p></li><li><p><a href="https://sites.google.com/view/icml19metalearning" target="_blank" rel="noopener noreferrer">ICML 2019 Tutorial - Meta-Learning: from Few-Shot Learning to Rapid Reinforcement Learning</a></p></li><li><p><a href="https://annotation-efficient-learning.github.io/" target="_blank" rel="noopener noreferrer">CVPR 2020 Tutorial - Towards Annotation-Efficient Learning: Few-Shot, Self-Supervised, and Incremental Learning Approaches</a></p></li><li><p><strong>Stanford CS330: Deep Multi-Task and Meta Learning.</strong> <em><a href="https://ai.stanford.edu/~cbfinn/" target="_blank" rel="noopener noreferrer">Chelsea Finn</a>.</em> [Video: <a href="https://www.bilibili.com/video/BV1He411s7K4" target="_blank" rel="noopener noreferrer">BiliBili</a>, <a href="https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5" target="_blank" rel="noopener noreferrer">YouTube</a>] <a href="https://cs330.stanford.edu/" target="_blank" rel="noopener noreferrer">[Homepage]</a></p></li><li><p><a href="https://www.dropbox.com/s/sm68skkkbxbob0i/metalearning.pdf?dl=0" target="_blank" rel="noopener noreferrer">Generalizing from Few Examples with Meta-Learning.</a> <em>Hugo Larochelle.</em></p></li></ul><h2 id="surveys" tabindex="-1"><a class="header-anchor" href="#surveys" aria-hidden="true">#</a> Surveys</h2><ul><li><strong>Meta-Learning: A Survey.</strong> <em>Joaquin Vanschoren.</em> arXiv 2018. <a href="https://arxiv.org/pdf/1810.03548.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></li></ul><h2 id="blogs-communities" tabindex="-1"><a class="header-anchor" href="#blogs-communities" aria-hidden="true">#</a> Blogs &amp; Communities</h2><ul><li><p><a href="http://metalearning-symposium.ml/" target="_blank" rel="noopener noreferrer">Metalearning Symposium, NIPS 2017</a></p></li><li><p><a href="https://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/" target="_blank" rel="noopener noreferrer"><strong>Learning to Learn.</strong></a> <em>Chelsea Finn.</em> BAIR blog, 2017.</p></li><li><p><strong>Learning to Learn Fast.</strong> <em>Lilian Weng.</em> 2018. <a href="https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html" target="_blank" rel="noopener noreferrer">[English]</a> <a href="https://wei-tianhao.github.io/blog/2019/09/17/meta-learning.html" target="_blank" rel="noopener noreferrer">[Chinese]</a></p></li><li><p><a href="https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html" target="_blank" rel="noopener noreferrer"><strong>Meta Reinforcement Learning.</strong></a> <em>Lilian Weng.</em> 2019.</p></li></ul><h2 id="theses" tabindex="-1"><a class="header-anchor" href="#theses" aria-hidden="true">#</a> Theses</h2><ul><li><strong>Learning to Learn with Gradient.</strong> <em><a href="https://ai.stanford.edu/~cbfinn/" target="_blank" rel="noopener noreferrer">Chelsea Finn</a>.</em> UC Berkeley, 2018. <a href="http://ai.stanford.edu/~cbfinn/_files/dissertation.pdf" target="_blank" rel="noopener noreferrer">[Thesis]</a></li></ul><h2 id="talks-slides" tabindex="-1"><a class="header-anchor" href="#talks-slides" aria-hidden="true">#</a> Talks &amp; Slides</h2><ul><li><p><a href="https://ai.stanford.edu/~cbfinn/_files/icml2018_automl_35min.pdf" target="_blank" rel="noopener noreferrer">Properties of Good Meta-Learning Algorithms (And How to Achieve Them).</a> <em><a href="https://ai.stanford.edu/~cbfinn/" target="_blank" rel="noopener noreferrer">Chelsea Finn</a>.</em> ICML 2018 AutoML Workshop.</p></li><li><p><a href="https://www.slideshare.net/YoonhoLee4/on-firstorder-metalearning-algorithms" target="_blank" rel="noopener noreferrer">Bayesian Model-Agnostic Meta-Learning</a></p></li></ul><h2 id="approaches" tabindex="-1"><a class="header-anchor" href="#approaches" aria-hidden="true">#</a> Approaches</h2><h3 id="gradient-based" tabindex="-1"><a class="header-anchor" href="#gradient-based" aria-hidden="true">#</a> Gradient-based</h3><ul><li><p><strong>Learning to Learn by Gradient Descent by Gradient Descent.</strong> <em>Marcin Andrychowicz, et al.</em> NIPS 2016. <a href="https://arxiv.org/pdf/1606.04474v1.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/deepmind/learning-to-learn" target="_blank" rel="noopener noreferrer">[Code]</a></p></li><li><p><strong>Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.</strong> <em>Chelsea Finn, et al.</em> ICML 2017. <a href="https://arxiv.org/pdf/1703.03400.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/cbfinn/maml" target="_blank" rel="noopener noreferrer">[Code]</a></p></li><li><p><strong>On First-Order Meta-Learning Algorithms.</strong> <em>Alex Nichol, et al.</em> arXiv 2018. <a href="https://arxiv.org/pdf/1803.02999.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a><a href="https://github.com/openai/supervised-reptile" target="_blank" rel="noopener noreferrer">[Code]</a></p></li><li><p><strong>Meta-Learning with Implicit Gradients.</strong> <em>Aravind Rajeswaran, et al.</em> NIPS 2019. <a href="https://arxiv.org/pdf/1909.04630.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li></ul><h3 id="sechond-order-derivative" tabindex="-1"><a class="header-anchor" href="#sechond-order-derivative" aria-hidden="true">#</a> Sechond-order Derivative</h3><ul><li><p><strong>Meta-Learning with Implicit Gradients.</strong> <em>Aravind Rajeswaran, et al.</em> NIPS 2019. <a href="https://arxiv.org/pdf/1909.04630.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://paperswithcode.com/paper/meta-learning-with-implicit-gradients" target="_blank" rel="noopener noreferrer">[Re-implementation]</a></p></li><li><p><strong>Efficient Meta Learning via Minibatch Proximal Update.</strong> <em>Pan Zhou, et al.</em> NIPS 2019. <a href="https://panzhous.github.io/assets/pdf/2019-NIPS-metaleanring.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://panzhous.github.io/assets/pdf/2019-NIPS-metaleanring-supplementary.pdf" target="_blank" rel="noopener noreferrer">[Appendix]</a> <a href="https://panzhous.github.io/assets/pdf/2019neurips-slides.pdf" target="_blank" rel="noopener noreferrer">[Slide]</a> <a href="https://panzhous.github.io/assets/pdf/2019-NIPS-poster.pdf" target="_blank" rel="noopener noreferrer">[Poster]</a> <a href="https://panzhous.github.io/assets/code/MetaMinibatchProx.zip" target="_blank" rel="noopener noreferrer">[Code]</a></p></li><li><p><strong>Meta-Curvature.</strong> <em>Eunbyung Park and Junier B. Oliva.</em> NIPS 2019. <a href="https://arxiv.org/pdf/1902.03356.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/silverbottlep/meta_curvature" target="_blank" rel="noopener noreferrer">[Code]</a></p></li></ul><h3 id="online" tabindex="-1"><a class="header-anchor" href="#online" aria-hidden="true">#</a> Online</h3><ul><li><p><strong>Online Learning of a Memory for Learning Rates.</strong> <em>Franziska Meier, et al.</em> ICRA 2018. <a href="https://arxiv.org/pdf/1709.06709.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/fmeier/online-meta-learning" target="_blank" rel="noopener noreferrer">[Code]</a></p></li><li><p><strong>Online Meta-Learning.</strong> <em>Chelsea Finn, et al.</em> ICML 2019. <a href="https://arxiv.org/pdf/1902.08438.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li></ul><h3 id="bayesian" tabindex="-1"><a class="header-anchor" href="#bayesian" aria-hidden="true">#</a> Bayesian</h3><ul><li><p><strong>Recasting Gradient-Based Meta-Learning as Hierarchical Bayes.</strong> <em>Erin Grant, et al.</em> ICLR 2018. <a href="https://arxiv.org/pdf/1801.08930.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li><li><p><strong>Bayesian Model-Agnostic Meta-Learning.</strong> <em>Taesup Kim, et al.</em> NIPS 2018. <a href="https://arxiv.org/pdf/1806.03836.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/jsikyoon/bmaml" target="_blank" rel="noopener noreferrer">[Code]</a> <a href="https://www.slideshare.net/sangwoomo7/bayesian-modelagnostic-metalearning" target="_blank" rel="noopener noreferrer">[Slide]</a></p></li><li><p><strong>Probabilistic Model-Agnostic Meta-Learning.</strong> <em>Chelsea Finn, et al.</em> arXiv 2018. <a href="https://arxiv.org/pdf/1806.02817.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li><li><p><strong>Uncertainty in Model-Agnostic Meta-Learning using Variational Inference.</strong> <em>Cuong Nguyen, et al.</em> WACV 2020. <a href="https://arxiv.org/pdf/1907.11864.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/cnguyen10/few_shot_meta_learning" target="_blank" rel="noopener noreferrer">[Code]</a></p></li><li><p><strong>Bayesian Online Meta-Learning with Laplace Approximation.</strong> <em>Pau Ching Yap, et al.</em> arXiv 2020. <a href="https://arxiv.org/pdf/2005.00146.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li></ul><h3 id="others" tabindex="-1"><a class="header-anchor" href="#others" aria-hidden="true">#</a> Others</h3><ul><li><strong>Provable Guarantees for Gradient-Based Meta-Learning.</strong> <em>Mikhail Khodak, et al.</em> ICML 2019. <a href="https://arxiv.org/pdf/1902.10644.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/mkhodak/FMRL" target="_blank" rel="noopener noreferrer">[Code]</a></li></ul><h2 id="libraries" tabindex="-1"><a class="header-anchor" href="#libraries" aria-hidden="true">#</a> Libraries</h2><ul><li><a href="https://github.com/learnables/learn2learn" target="_blank" rel="noopener noreferrer">learn2learn</a></li><li><a href="https://github.com/tristandeleu/pytorch-meta" target="_blank" rel="noopener noreferrer">Torchmeta</a></li></ul>',27);function t(o,i){return n}var h=r(a,[["render",t]]);export{h as default};
