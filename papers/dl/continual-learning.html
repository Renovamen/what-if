<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="generator" content="VuePress 2.0.0-beta.38">
    <style>
      :root {
        --c-bg: #fff;
      }
      html.dark {
        --c-bg: #22272e;
      }
      html, body {
        background-color: var(--c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem('vuepress-color-scheme');
			const systemDarkMode = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
			if (userMode === 'dark' || (userMode !== 'light' && systemDarkMode)) {
				document.documentElement.classList.toggle('dark', true);
			}
    </script>
    <link rel="icon" href="/favicon.svg"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><title>Continual Learning | What if?</title><meta name="description" content="Renovamen's messy notebook.">
    <link rel="modulepreload" href="/assets/app.dd6d0129.js"><link rel="modulepreload" href="/assets/continual-learning.html.7b10620a.js"><link rel="modulepreload" href="/assets/continual-learning.html.4c4d1962.js">
    <link rel="stylesheet" href="/assets/style.f9bcbabd.css">
  </head>
  <body>
    <div id="app"><!--[--><div class="theme-container"><!--[--><header class="navbar"><div class="toggle-sidebar-button" title="toggle sidebar" aria-expanded="false" role="button" tabindex="0"><div class="icon" aria-hidden="true"><span></span><span></span><span></span></div></div><span><a href="/" class=""><img class="logo" src="/favicon.svg" alt="What if?"><span class="site-name can-hide">What if?</span></a></span><div class="navbar-items-wrapper" style=""><!--[--><!--]--><nav class="navbar-items can-hide"><!--[--><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Stupid AI"><span class="title">Stupid AI</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Stupid AI"><span class="title">Stupid AI</span><span class="right arrow"></span></button><!--[--><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a href="/ai/ml/" class="" aria-label="Machine Learning"><!--[--><!--]--> Machine Learning <!--[--><!--]--></a></li><li class="navbar-dropdown-item"><a href="/ai/dl/" class="" aria-label="Deep Learning"><!--[--><!--]--> Deep Learning <!--[--><!--]--></a></li><li class="navbar-dropdown-item"><a href="/ai/rl/" class="" aria-label="Reinforcement Learning"><!--[--><!--]--> Reinforcement Learning <!--[--><!--]--></a></li><li class="navbar-dropdown-item"><a href="/ai/nlp/" class="" aria-label="Statistical NLP"><!--[--><!--]--> Statistical NLP <!--[--><!--]--></a></li><!--]--></ul><!--]--></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Math"><span class="title">Math</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Math"><span class="title">Math</span><span class="right arrow"></span></button><!--[--><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a href="/math/linear-algebra/" class="" aria-label="Linear Algebra"><!--[--><!--]--> Linear Algebra <!--[--><!--]--></a></li><li class="navbar-dropdown-item"><a href="/math/information-theory/" class="" aria-label="Information Theory"><!--[--><!--]--> Information Theory <!--[--><!--]--></a></li><!--]--></ul><!--]--></div></div><div class="navbar-item"><a href="/dev/" class="" aria-label="Dev"><!--[--><!--]--> Dev <!--[--><!--]--></a></div><div class="navbar-item"><a href="/papers/" class="router-link-active" aria-label="Papers"><!--[--><!--]--> Papers <!--[--><!--]--></a></div><div class="navbar-item"><a href="/snippets/" class="" aria-label="Snippets"><!--[--><!--]--> Snippets <!--[--><!--]--></a></div><!--]--></nav><!--[--><!--]--><div class="navbar-buttons"><a rel="noopener noreferrer" target="_blank" href="https://github.com/Renovamen/what-if"><svg class="ov-icon" style="font-size:1.26em;" aria-hidden="true" width="20.16" height="20.16" viewbox="0 0 24 24" fill="currentColor"><path fill="none" d="M0 0h24v24H0z"/><path d="M5.883 18.653c-.3-.2-.558-.455-.86-.816a50.32 50.32 0 01-.466-.579c-.463-.575-.755-.84-1.057-.949a1 1 0 01.676-1.883c.752.27 1.261.735 1.947 1.588-.094-.117.34.427.433.539.19.227.33.365.44.438.204.137.587.196 1.15.14.023-.382.094-.753.202-1.095C5.38 15.31 3.7 13.396 3.7 9.64c0-1.24.37-2.356 1.058-3.292-.218-.894-.185-1.975.302-3.192a1 1 0 01.63-.582c.081-.024.127-.035.208-.047.803-.123 1.937.17 3.415 1.096A11.731 11.731 0 0112 3.315c.912 0 1.818.104 2.684.308 1.477-.933 2.613-1.226 3.422-1.096.085.013.157.03.218.05a1 1 0 01.616.58c.487 1.216.52 2.297.302 3.19.691.936 1.058 2.045 1.058 3.293 0 3.757-1.674 5.665-4.642 6.392.125.415.19.879.19 1.38a300.492 300.492 0 01-.012 2.716 1 1 0 01-.019 1.958c-1.139.228-1.983-.532-1.983-1.525l.002-.446.005-.705c.005-.708.007-1.338.007-1.998 0-.697-.183-1.152-.425-1.36-.661-.57-.326-1.655.54-1.752 2.967-.333 4.337-1.482 4.337-4.66 0-.955-.312-1.744-.913-2.404a1 1 0 01-.19-1.045c.166-.414.237-.957.096-1.614l-.01.003c-.491.139-1.11.44-1.858.949a1 1 0 01-.833.135A9.626 9.626 0 0012 5.315c-.89 0-1.772.119-2.592.35a1 1 0 01-.83-.134c-.752-.507-1.374-.807-1.868-.947-.144.653-.073 1.194.092 1.607a1 1 0 01-.189 1.045C6.016 7.89 5.7 8.694 5.7 9.64c0 3.172 1.371 4.328 4.322 4.66.865.097 1.201 1.177.544 1.748-.192.168-.429.732-.429 1.364v3.15c0 .986-.835 1.725-1.96 1.528a1 1 0 01-.04-1.962v-.99c-.91.061-1.662-.088-2.254-.485z"/></svg></a><a rel="noopener noreferrer" target="_blank" href="https://zxh.io"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewbox="0 0 24 24" fill="currentColor"><path fill="none" d="M0 0h24v24H0z"/><path d="M13.12 17.023l-4.199-2.29a4 4 0 110-5.465l4.2-2.29a4 4 0 11.959 1.755l-4.2 2.29a4.008 4.008 0 010 1.954l4.199 2.29a4 4 0 11-.959 1.755zM6 14a2 2 0 100-4 2 2 0 000 4zm11-6a2 2 0 100-4 2 2 0 000 4zm0 12a2 2 0 100-4 2 2 0 000 4z"/></svg></a><button class="toggle-dark-button" title="toggle dark mode"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewbox="0 0 24 24" fill="currentColor" focusable="false"><path fill="none" d="M0 0h24v24H0z"/><path d="M12 18a6 6 0 110-12 6 6 0 010 12zm0-2a4 4 0 100-8 4 4 0 000 8zM11 1h2v3h-2V1zm0 19h2v3h-2v-3zM3.515 4.929l1.414-1.414L7.05 5.636 5.636 7.05 3.515 4.93zM16.95 18.364l1.414-1.414 2.121 2.121-1.414 1.414-2.121-2.121zm2.121-14.85l1.414 1.415-2.121 2.121-1.414-1.414 2.121-2.121zM5.636 16.95l1.414 1.414-2.121 2.121-1.414-1.414 2.121-2.121zM23 11v2h-3v-2h3zM4 11v2H1v-2h3z"/></svg><svg class="ov-icon" style="font-size:1.2em;display:none;" aria-hidden="true" width="19.2" height="19.2" viewbox="0 0 24 24" fill="currentColor" focusable="false"><path fill="none" d="M0 0h24v24H0z"/><path d="M10 7a7 7 0 0012 4.9v.1c0 5.523-4.477 10-10 10S2 17.523 2 12 6.477 2 12 2h.1A6.979 6.979 0 0010 7zm-6 5a8 8 0 0015.062 3.762A9 9 0 018.238 4.938 7.999 7.999 0 004 12z"/></svg></button></div><form class="search-box" role="search"><input type="search" autocomplete="off" spellcheck="false" value><!----></form></div></header><!--]--><div class="sidebar-mask"></div><!--[--><aside class="sidebar"><nav class="navbar-items"><!--[--><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Stupid AI"><span class="title">Stupid AI</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Stupid AI"><span class="title">Stupid AI</span><span class="right arrow"></span></button><!--[--><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a href="/ai/ml/" class="" aria-label="Machine Learning"><!--[--><!--]--> Machine Learning <!--[--><!--]--></a></li><li class="navbar-dropdown-item"><a href="/ai/dl/" class="" aria-label="Deep Learning"><!--[--><!--]--> Deep Learning <!--[--><!--]--></a></li><li class="navbar-dropdown-item"><a href="/ai/rl/" class="" aria-label="Reinforcement Learning"><!--[--><!--]--> Reinforcement Learning <!--[--><!--]--></a></li><li class="navbar-dropdown-item"><a href="/ai/nlp/" class="" aria-label="Statistical NLP"><!--[--><!--]--> Statistical NLP <!--[--><!--]--></a></li><!--]--></ul><!--]--></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Math"><span class="title">Math</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Math"><span class="title">Math</span><span class="right arrow"></span></button><!--[--><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a href="/math/linear-algebra/" class="" aria-label="Linear Algebra"><!--[--><!--]--> Linear Algebra <!--[--><!--]--></a></li><li class="navbar-dropdown-item"><a href="/math/information-theory/" class="" aria-label="Information Theory"><!--[--><!--]--> Information Theory <!--[--><!--]--></a></li><!--]--></ul><!--]--></div></div><div class="navbar-item"><a href="/dev/" class="" aria-label="Dev"><!--[--><!--]--> Dev <!--[--><!--]--></a></div><div class="navbar-item"><a href="/papers/" class="router-link-active" aria-label="Papers"><!--[--><!--]--> Papers <!--[--><!--]--></a></div><div class="navbar-item"><a href="/snippets/" class="" aria-label="Snippets"><!--[--><!--]--> Snippets <!--[--><!--]--></a></div><!--]--></nav><!--[--><!--]--><ul class="sidebar-items"><!--[--><li><p tabindex="0" class="sidebar-item sidebar-heading">NLP <!----></p><!--[--><ul style="" class="sidebar-item-children"><!--[--><li><a href="/papers/nlp/kg-application.html" class="sidebar-item" aria-label="Application of KGs"><!--[--><!--]--> Application of KGs <!--[--><!--]--></a><!----></li><!--]--></ul><!--]--></li><li><p tabindex="0" class="sidebar-item sidebar-heading">CV <!----></p><!--[--><ul style="" class="sidebar-item-children"><!--[--><li><a href="/papers/cv/image-aesthetic-assessment.html" class="sidebar-item" aria-label="Image Aesthetic Assessment"><!--[--><!--]--> Image Aesthetic Assessment <!--[--><!--]--></a><!----></li><li><a href="/papers/cv/reid.html" class="sidebar-item" aria-label="Person Re-Identification"><!--[--><!--]--> Person Re-Identification <!--[--><!--]--></a><!----></li><!--]--></ul><!--]--></li><li><p tabindex="0" class="sidebar-item sidebar-heading active">DL <!----></p><!--[--><ul style="" class="sidebar-item-children"><!--[--><li><a aria-current="page" href="/papers/dl/continual-learning.html" class="router-link-active router-link-exact-active router-link-active sidebar-item active" aria-label="Continual Learning"><!--[--><!--]--> Continual Learning <!--[--><!--]--></a><!--[--><ul style="" class="sidebar-item-children"><!--[--><li><a aria-current="page" href="/papers/dl/continual-learning.html#other-awesome-lists" class="router-link-active router-link-exact-active sidebar-item" aria-label="Other Awesome Lists"><!--[--><!--]--> Other Awesome Lists <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/papers/dl/continual-learning.html#courses-tutorials" class="router-link-active router-link-exact-active sidebar-item" aria-label="Courses &amp; Tutorials"><!--[--><!--]--> Courses &amp; Tutorials <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/papers/dl/continual-learning.html#surveys" class="router-link-active router-link-exact-active sidebar-item" aria-label="Surveys"><!--[--><!--]--> Surveys <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/papers/dl/continual-learning.html#theses" class="router-link-active router-link-exact-active sidebar-item" aria-label="Theses"><!--[--><!--]--> Theses <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/papers/dl/continual-learning.html#blogs-communities" class="router-link-active router-link-exact-active sidebar-item" aria-label="Blogs &amp; Communities"><!--[--><!--]--> Blogs &amp; Communities <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/papers/dl/continual-learning.html#approaches" class="router-link-active router-link-exact-active sidebar-item" aria-label="Approaches"><!--[--><!--]--> Approaches <!--[--><!--]--></a><!--[--><ul style="" class="sidebar-item-children"><!--[--><li><a aria-current="page" href="/papers/dl/continual-learning.html#regularization" class="router-link-active router-link-exact-active sidebar-item" aria-label="Regularization"><!--[--><!--]--> Regularization <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/papers/dl/continual-learning.html#rehearsal" class="router-link-active router-link-exact-active sidebar-item" aria-label="Rehearsal"><!--[--><!--]--> Rehearsal <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/papers/dl/continual-learning.html#dynamic-expansion" class="router-link-active router-link-exact-active sidebar-item" aria-label="Dynamic Expansion"><!--[--><!--]--> Dynamic Expansion <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/papers/dl/continual-learning.html#task-free" class="router-link-active router-link-exact-active sidebar-item" aria-label="Task Free"><!--[--><!--]--> Task Free <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/papers/dl/continual-learning.html#meta-learning" class="router-link-active router-link-exact-active sidebar-item" aria-label="+ Meta Learning"><!--[--><!--]--> + Meta Learning <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/papers/dl/continual-learning.html#reinforcement-learning" class="router-link-active router-link-exact-active sidebar-item" aria-label="+ Reinforcement Learning"><!--[--><!--]--> + Reinforcement Learning <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/papers/dl/continual-learning.html#generative-modeling" class="router-link-active router-link-exact-active sidebar-item" aria-label="+ Generative Modeling"><!--[--><!--]--> + Generative Modeling <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/papers/dl/continual-learning.html#bayesian" class="router-link-active router-link-exact-active sidebar-item" aria-label="Bayesian"><!--[--><!--]--> Bayesian <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/papers/dl/continual-learning.html#new-settings" class="router-link-active router-link-exact-active sidebar-item" aria-label="New Settings"><!--[--><!--]--> New Settings <!--[--><!--]--></a><!----></li><!--]--></ul><!--]--></li><li><a aria-current="page" href="/papers/dl/continual-learning.html#workshops" class="router-link-active router-link-exact-active sidebar-item" aria-label="Workshops"><!--[--><!--]--> Workshops <!--[--><!--]--></a><!----></li><!--]--></ul><!--]--></li><li><a href="/papers/dl/meta-learning.html" class="sidebar-item" aria-label="Meta Learning / Few-Shot Learning"><!--[--><!--]--> Meta Learning / Few-Shot Learning <!--[--><!--]--></a><!----></li><li><a href="/papers/dl/model-compression.html" class="sidebar-item" aria-label="Model Compression"><!--[--><!--]--> Model Compression <!--[--><!--]--></a><!----></li><li><a href="/papers/dl/misc.html" class="sidebar-item" aria-label="Misc"><!--[--><!--]--> Misc <!--[--><!--]--></a><!----></li><!--]--></ul><!--]--></li><!--]--></ul><!--[--><!--]--></aside><!--]--><div class="page-content"><!--[--><main class="page"><!--[--><!--]--><div class="theme-default-content"><!--[--><h1 id="continual-learning" tabindex="-1"><a class="header-anchor" href="#continual-learning" aria-hidden="true">#</a> Continual Learning</h1><p>Literatures of Continual Learning (持续学习, also called Lifelong / Incremental / Cumulative Learning).</p><h2 id="other-awesome-lists" tabindex="-1"><a class="header-anchor" href="#other-awesome-lists" aria-hidden="true">#</a> Other Awesome Lists</h2><ul><li><a href="https://wiki.continualai.org/index.html" target="_blank" rel="noopener noreferrer">ContinualAI Wiki</a></li><li><a href="https://github.com/optimass/continual_learning_papers" target="_blank" rel="noopener noreferrer">optimass/continual_learning_papers</a></li><li><a href="https://github.com/xialeiliu/Awesome-Incremental-Learning" target="_blank" rel="noopener noreferrer">xialeiliu/Awesome-Incremental-Learning</a></li><li><a href="https://github.com/prprbr/awesome-lifelong-continual-learning" target="_blank" rel="noopener noreferrer">prprbr/awesome-lifelong-continual-learning</a></li><li><a href="https://github.com/floodsung/Lifelong-Learning-Paper-List" target="_blank" rel="noopener noreferrer">floodsung/Lifelong-Learning-Paper-List</a></li></ul><h2 id="courses-tutorials" tabindex="-1"><a class="header-anchor" href="#courses-tutorials" aria-hidden="true">#</a> Courses &amp; Tutorials</h2><ul><li><p><strong>Life Long Learning.</strong> Hung-yi Lee (李宏毅). <a href="https://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Lifelong%20Learning%20(v9).pdf" target="_blank" rel="noopener noreferrer">[Slide]</a> <a href="https://www.youtube.com/watch?v=7qT5P9KJnWo&amp;list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&amp;index=25" target="_blank" rel="noopener noreferrer">[Video]</a></p></li><li><p><a href="https://annotation-efficient-learning.github.io/" target="_blank" rel="noopener noreferrer">CVPR 2020 Tutorial - Towards Annotation-Efficient Learning: Few-Shot, Self-Supervised, and Incremental Learning Approaches</a></p></li></ul><h2 id="surveys" tabindex="-1"><a class="header-anchor" href="#surveys" aria-hidden="true">#</a> Surveys</h2><ul><li><p><strong>Continual Lifelong Learning with Neural Networks: A Review.</strong> <em>German I. Parisi, et al.</em> Neural Networks 2019. <a href="https://arxiv.org/pdf/1802.07569.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p><blockquote><p>A survey on different approaches (regularization / dynamic architectures / rehearsal) for continual learning.</p></blockquote></li><li><p><strong>Three Scenarios for Continual Learning.</strong> <em>Gido M. van de Ven and Andreas S. Tolias.</em> arXiv 2019. <a href="https://arxiv.org/pdf/1904.07734.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/GMvandeVen/continual-learning" target="_blank" rel="noopener noreferrer">[Code]</a></p><blockquote><p>A survey on three different scenarios (task / domain / class) for continual learning.</p></blockquote></li><li><p><strong>Continual Learning: A Comparative Study on How to Defy Forgetting in Classification Tasks.</strong> <em>Matthias De Lange, et al.</em> arXiv 2019. <a href="https://arxiv.org/pdf/1909.08383.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li><li><p><strong>Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challenges.</strong> <em>Timothée Lesort, et al.</em> Inf. Fusion 2020. <a href="https://www.sciencedirect.com/science/article/pii/S1566253519307377" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li></ul><h2 id="theses" tabindex="-1"><a class="header-anchor" href="#theses" aria-hidden="true">#</a> Theses</h2><ul><li><p><strong>Continual Learning with Deep Architectures.</strong> <em>Vincenzo Lomonaco.</em> University of Bologna, 2019. <a href="http://amsdottorato.unibo.it/9073/1/vincenzo_lomonaco_thesis.pdf" target="_blank" rel="noopener noreferrer">[Thesis]</a></p></li><li><p><strong>Continual Learning with Neural Networks.</strong> <em><a href="https://saynaebrahimi.github.io/" target="_blank" rel="noopener noreferrer">Sayna Ebrahimi</a>.</em> UC Berkeley, 2020. <a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2020/EECS-2020-82.pdf" target="_blank" rel="noopener noreferrer">[Thesis]</a></p></li><li><p><strong>Continual Learning: Tackling Catastrophic Forgetting in Deep Neural Networks with Replay Processes.</strong> <em>Timothée Lesort.</em> ENSTA-ParisTech, 2020. <a href="https://arxiv.org/pdf/2007.00487.pdf" target="_blank" rel="noopener noreferrer">[Thesis]</a></p></li></ul><h2 id="blogs-communities" tabindex="-1"><a class="header-anchor" href="#blogs-communities" aria-hidden="true">#</a> Blogs &amp; Communities</h2><ul><li><p><a href="https://vlomonaco.github.io/core50/strategies.html" target="_blank" rel="noopener noreferrer">Continuous Learning Strategies (CORe50 Dataset)</a></p></li><li><p><a href="https://medium.com/continual-ai/why-continuous-learning-is-the-key-towards-machine-intelligence-1851cb57c308" target="_blank" rel="noopener noreferrer">Why Continual Learning is the key towards Machine Intelligence.</a> <em>Vincenzo Lomonaco.</em> Medium, 2017.</p></li><li><p><a href="https://www.continualai.org/" target="_blank" rel="noopener noreferrer">ContinualAI</a></p></li></ul><h2 id="approaches" tabindex="-1"><a class="header-anchor" href="#approaches" aria-hidden="true">#</a> Approaches</h2><h3 id="regularization" tabindex="-1"><a class="header-anchor" href="#regularization" aria-hidden="true">#</a> Regularization</h3><p>Impose constraints on the update of the neural weights.</p><ul><li><p><strong>Learning without Forgetting.</strong> <em>Zhizhong Li and Derek Hoiem.</em> ECCV 2016. <a href="https://arxiv.org/pdf/1606.09282.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/lizhitwo/LearningWithoutForgetting" target="_blank" rel="noopener noreferrer">[Code]</a> (LwF)</p></li><li><p><strong>Overcoming Catastrophic Forgetting in Neural Networks.</strong> <em>James Kirkpatrick, et al.</em> PNAS 2017. <a href="https://arxiv.org/pdf/1612.00796.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> (Elastic Weight Consolidation, EWC)</p></li><li><p><strong>Continual Learning Through Synaptic Intelligence.</strong> <em>Friedemann Zenke, et al.</em> ICML 2017. <a href="https://arxiv.org/pdf/1703.04200.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/ganguli-lab/pathint" target="_blank" rel="noopener noreferrer">[Code]</a> (Intelligent Synapses, IS)</p></li><li><p><strong>Online Structured Laplace Approximations For Overcoming Catastrophic Forgetting.</strong> <em>Hippolyt Ritter, et al.</em> NIPS 2018. <a href="https://arxiv.org/pdf/1805.07810.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li><li><p><strong>Improving and Understanding Variational Continual Learning.</strong> <em>Siddharth Swaroop, et al.</em> NIPS 2018 Continual Learning Workshop. <a href="https://arxiv.org/pdf/1905.02099.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/nvcuong/variational-continual-learning/tree/master/improved_ddm" target="_blank" rel="noopener noreferrer">[Code]</a></p></li><li><p><strong>Memory Aware Synapses: Learning What (Not) to Forget.</strong> <em><a href="https://homes.esat.kuleuven.be/~raljundi/index.html" target="_blank" rel="noopener noreferrer">Rahaf Aljundi</a>, et al.</em> ECCV 2018. <a href="https://arxiv.org/pdf/1711.09601.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/rahafaljundi/MAS-Memory-Aware-Synapses" target="_blank" rel="noopener noreferrer">[Code]</a></p></li><li><p><strong>Task Agnostic Continual Learning Using Online Variational Bayes.</strong> <em>Chen Zeno, et al.</em> arXiv 2018. <a href="https://arxiv.org/pdf/1803.10123.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/igolan/bgd" target="_blank" rel="noopener noreferrer">[Code]</a></p></li><li><p><strong>Task-Free Continual Learning.</strong> <em><a href="https://homes.esat.kuleuven.be/~raljundi/index.html" target="_blank" rel="noopener noreferrer">Rahaf Aljundi</a>, et al.</em> CVPR 2019. <a href="https://arxiv.org/pdf/1812.03596.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li><li><p><strong>Online Continual Learning with Maximally Interfered Retrieval.</strong> <em><a href="https://homes.esat.kuleuven.be/~raljundi/index.html" target="_blank" rel="noopener noreferrer">Rahaf Aljundi</a>, et al.</em> NIPS 2019. <a href="https://papers.nips.cc/paper/9357-online-continual-learning-with-maximal-interfered-retrieval.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li><li><p><strong>Uncertainty-based Continual Learning with Adaptive Regularization.</strong> <em>Hongjoon Ahn, et al.</em> NIPS 2019. <a href="https://arxiv.org/pdf/1905.11614.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/csm9493/UCL" target="_blank" rel="noopener noreferrer">[Code]</a></p></li><li><p><strong>Efficient continual learning in neural networks with embedding regularization.</strong> <em>Jary Pomponi, et al.</em> Neurocomputing 2020. <a href="https://arxiv.org/pdf/1909.03742.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li><li><p><strong>Uncertainty-guided Ccontinual Learning with Bayesian Neural Networks.</strong> <em>Sayna Ebrahimi, et al.</em> ICLR 2020. <a href="https://arxiv.org/pdf/1906.02425.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/SaynaEbrahimi/UCB" target="_blank" rel="noopener noreferrer">[Code]</a></p></li><li><p><strong>Continual Learning with Node-Importance based Adaptive Group Sparse Regularization.</strong> <em>Sangwon Jung, et al.</em> CVPR 2020 Workshop on Continual Learning in Computer Vision. <a href="https://arxiv.org/pdf/2003.13726.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li><li><p><strong>SOLA: Continual Learning with Second-Order Loss Approximation.</strong> <em>Dong Yin, et al.</em> arXiv 2020. <a href="https://arxiv.org/pdf/2006.10974v1.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li><li><p><strong>CPR: Classifier-Projection Regularization for Continual Learning.</strong> <em>Sungmin Cha, et al.</em> ICLR 2021. <a href="https://openreview.net/pdf?id=F2v4aqEL6ze" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/csm9493/CPR_CL" target="_blank" rel="noopener noreferrer">[Code]</a></p></li></ul><h3 id="rehearsal" tabindex="-1"><a class="header-anchor" href="#rehearsal" aria-hidden="true">#</a> Rehearsal</h3><h4 id="extra-memory" tabindex="-1"><a class="header-anchor" href="#extra-memory" aria-hidden="true">#</a> Extra Memory</h4><p>Use extra memory to store data from previous tasks.</p><ul><li><p><strong>Gradient Episodic Memory for Continual Learning.</strong> <em>David Lopez-Paz and Marc&#39;Aurelio Ranzato.</em> NIPS 2017. <a href="https://papers.nips.cc/paper/7225-gradient-episodic-memory-for-continual-learning.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/facebookresearch/GradientEpisodicMemory" target="_blank" rel="noopener noreferrer">[Code]</a></p></li><li><p><strong>iCaRL: Incremental Classifier and Representation Learning.</strong> <em>Sylvestre-Alvise Rebuffi, et al.</em> CVPR 2017. <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Rebuffi_iCaRL_Incremental_Classifier_CVPR_2017_paper.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/srebuffi/iCaRL" target="_blank" rel="noopener noreferrer">[Code]</a></p></li><li><p><strong>Variational Continual Learning.</strong> <em>Cuong V. Nguyen, et al.</em> ICLR 2018. <a href="https://openreview.net/pdf?id=BkQqq0gRb" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/nvcuong/variational-continual-learning" target="_blank" rel="noopener noreferrer">[Code]</a> (VCL)</p></li><li><p><strong>Experience Replay for Continual Learning.</strong> <em>David Rolnick, et al.</em> NIPS 2019. <a href="https://papers.nips.cc/paper/8327-experience-replay-for-continual-learning.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li><li><p><strong>Continual Learning with Bayesian Neural Networks for Non-Stationary Data.</strong> <em>Richard Kurle, et al.</em> ICLR 2020. <a href="https://openreview.net/pdf?id=SJlsFpVtDB" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li><li><p><strong>Graph-Based Continual Learning.</strong> <em>Binh Tang and David S. Matteson.</em> ICLR 2021. <a href="https://openreview.net/pdf?id=HHSEKOnPvaO" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li><li><p><strong>Gradient Projection Memory for Continual Learning.</strong> <em>Gobinda Saha, et al.</em> ICLR 2021. <a href="https://openreview.net/pdf?id=3AOj0RCNC2" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/sahagobinda/GPM" target="_blank" rel="noopener noreferrer">[Code]</a></p></li></ul><h4 id="generative-replay" tabindex="-1"><a class="header-anchor" href="#generative-replay" aria-hidden="true">#</a> Generative Replay</h4><p>Mimic past data by generative models (GAN, VAE, etc).</p><ul><li><p><strong>Continual Learning with Deep Generative Replay.</strong> <em>Hanul Shin, et al.</em> NIPS 2017. <a href="https://papers.nips.cc/paper/6892-continual-learning-with-deep-generative-replay.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li><li><p><strong>FearNet: Brain-Inspired Model for Incremental Learning.</strong> <em>Ronald Kemker and Christopher Kanan.</em> ICLR 2018. <a href="https://openreview.net/pdf?id=SJ1Xmf-Rb" target="_blank" rel="noopener noreferrer">[Code]</a></p></li><li><p><strong>Generative Models from the perspective of Continual Learning.</strong> <em>Timothée Lesort, et al.</em> IJCNN 2019. <a href="https://arxiv.org/pdf/1812.09111.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/TLESORT/Generative_Continual_Learning" target="_blank" rel="noopener noreferrer">[Code]</a></p></li><li><p><strong>Learning to Remember: A Synaptic Plasticity Driven Framework for Continual Learning.</strong> <em>Oleksiy Ostapenko, et al.</em> CVPR 2019. <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Ostapenko_Learning_to_Remember_A_Synaptic_Plasticity_Driven_Framework_for_Continual_CVPR_2019_paper.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/SAP-samples/machine-learning-dgm" target="_blank" rel="noopener noreferrer">[Code]</a></p></li><li><p><strong>Continual Unsupervised Representation Learning.</strong> <em>Dushyant Rao, et al.</em> NIPS 2019. <a href="https://papers.nips.cc/paper/8981-continual-unsupervised-representation-learning.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/deepmind/deepmind-research/tree/master/curl" target="_blank" rel="noopener noreferrer">[Code]</a></p><blockquote><p>Continual learning without task boundaries via dynamic expansion and generative replay (VAE).</p></blockquote></li></ul><h3 id="dynamic-expansion" tabindex="-1"><a class="header-anchor" href="#dynamic-expansion" aria-hidden="true">#</a> Dynamic Expansion</h3><p>Increase in network capacity that handles new tasks without affecting learned networks.</p><ul><li><p><strong>Net2Net: Accelerating Learning via Knowledge Transfer.</strong> <em>Tianqi Chen, et al.</em> ICLR 2016. <a href="https://arxiv.org/pdf/1511.05641.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li><li><p><strong>Progressive Neural Networks.</strong> <em>Andrei A. Rusu, et al.</em> arXiv 2016. <a href="https://arxiv.org/pdf/1606.04671.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li><li><p><strong>Expert Gate: Lifelong Learning with a Network of Experts.</strong> <em><a href="https://homes.esat.kuleuven.be/~raljundi/index.html" target="_blank" rel="noopener noreferrer">Rahaf Aljundi</a>, et al.</em> CVPR 2017. <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Aljundi_Expert_Gate_Lifelong_CVPR_2017_paper.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/wannabeOG/ExpertNet-Pytorch" target="_blank" rel="noopener noreferrer">[Re-implementation]</a></p></li><li><p><strong>Random Path Selection for Continual Learning.</strong> <em>Jathushan Rajasegaran, et al.</em> NIPS 2019. <a href="http://papers.nips.cc/paper/9429-random-path-selection-for-continual-learning.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/brjathu/RPSnet" target="_blank" rel="noopener noreferrer">[Code]</a></p></li><li><p><strong>Compacting, Picking and Growing for Unforgetting Continual Learning.</strong> <em>Steven C. Y. Hung, et al.</em> NIPS 2019. <a href="https://papers.nips.cc/paper/9518-compacting-picking-and-growing-for-unforgetting-continual-learning.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/ivclab/CPG" target="_blank" rel="noopener noreferrer">[Code]</a></p><blockquote><p>Gradual model pruning (compacting) → Train a 0-1 mask to pick weights of previous tasks (picking) → Dynamic expansion (growing)</p></blockquote></li><li><p><strong>Continual Unsupervised Representation Learning.</strong> <em>Dushyant Rao, et al.</em> NIPS 2019. <a href="https://papers.nips.cc/paper/8981-continual-unsupervised-representation-learning.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/deepmind/deepmind-research/tree/master/curl" target="_blank" rel="noopener noreferrer">[Code]</a></p><blockquote><p>Continual learning without task boundaries via dynamic expansion and generative replay (VAE).</p></blockquote></li><li><p><strong>Continual Learning with Adaptive Weights (CLAW).</strong> <em>Tameem Adel, et al.</em> ICLR 2020. <a href="https://openreview.net/pdf?id=Hklso24Kwr" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li><li><p><strong>A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning.</strong> <em>Soochan Lee, et al.</em> ICLR 2020. <a href="https://openreview.net/pdf?id=SJxSOJStPr" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/soochan-lee/CN-DPM" target="_blank" rel="noopener noreferrer">[Code]</a></p></li></ul><h3 id="task-free" tabindex="-1"><a class="header-anchor" href="#task-free" aria-hidden="true">#</a> Task Free</h3><ul><li><p><strong>Task Agnostic Continual Learning Using Online Variational Bayes.</strong> <em>Chen Zeno, et al.</em> arXiv 2018. <a href="https://arxiv.org/pdf/1803.10123.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/igolan/bgd" target="_blank" rel="noopener noreferrer">[Code]</a></p></li><li><p><strong>Task-Free Continual Learning.</strong> <em><a href="https://homes.esat.kuleuven.be/~raljundi/index.html" target="_blank" rel="noopener noreferrer">Rahaf Aljundi</a>, et al.</em> CVPR 2019. <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Aljundi_Task-Free_Continual_Learning_CVPR_2019_paper.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li><li><p><strong>Continual Unsupervised Representation Learning.</strong> <em>Dushyant Rao, et al.</em> NIPS 2019. <a href="https://papers.nips.cc/paper/8981-continual-unsupervised-representation-learning.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/deepmind/deepmind-research/tree/master/curl" target="_blank" rel="noopener noreferrer">[Code]</a></p><blockquote><p>Continual learning without task boundaries via dynamic expansion (Dirichlet process) and generative replay (VAE).</p></blockquote></li><li><p><strong>Reconciling Meta-Learning and Continual Learning with Online Mixtures of Tasks.</strong> <em>Ghassen Jerfel, et al.</em> NIPS 2019. <a href="https://arxiv.org/pdf/1812.06080.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li><li><p><strong>Continuous Meta-Learning without Tasks.</strong> <em>James Harrison, et al.</em> arXiv 2019. <a href="https://arxiv.org/pdf/1912.08866.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/StanfordASL/moca" target="_blank" rel="noopener noreferrer">[Code]</a> <a href="https://openreview.net/forum?id=r1l1myStwr" target="_blank" rel="noopener noreferrer">[OpenReview]</a></p><blockquote><p>Integrate Bayesian online changepoint detection algorithm with existing meta-learning approaches to enable meta-learning in task-unsegmented settings.</p></blockquote></li><li><p><strong>A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning.</strong> <em>Soochan Lee, et al.</em> ICLR 2020. <a href="https://openreview.net/pdf?id=SJxSOJStPr" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/soochan-lee/CN-DPM" target="_blank" rel="noopener noreferrer">[Code]</a></p></li><li><p><strong>Task Agnostic Continual Learning via Meta Learning.</strong> <em>Xu He, et al.</em> ICML 2020 LifelongML Workshop. <a href="https://openreview.net/pdf?id=AeIzVxdJgeb" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li></ul><h3 id="meta-learning" tabindex="-1"><a class="header-anchor" href="#meta-learning" aria-hidden="true">#</a> + Meta Learning</h3><p><a href="/papers/dl/meta-learning/" class="">Here</a> is also a list of literatures for Meta Learning.</p><ul><li><p><strong>Meta Continual Learning.</strong> <em>Risto Vuori, et al.</em> arXiv 2018. <a href="https://arxiv.org/pdf/1806.06928.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p><blockquote><p>Train a RNN as optimizer, and the optimizer leverages information of both current and previous tasks to learn to preserve previous parameters.</p></blockquote></li><li><p><strong>Learning to Learn without Forgetting by Maximizing Transfer and Minimizing Interference.</strong> <em>Matthew Riemer, et al.</em> ICLR 2019. <a href="https://arxiv.org/pdf/1810.11910.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/mattriemer/MER" target="_blank" rel="noopener noreferrer">[Code]</a> (Meta-Experience Replay, MER)</p><blockquote><p>Combine Reptile (a meta-learning algorithm) with experience replay for rapidly learning the current and future experience and preserving past knowledge.</p></blockquote></li><li><p><strong>Meta-Learning Representations for Continual Learning.</strong> <em>Khurram Javed and Martha White.</em> NIPS 2019. <a href="https://arxiv.org/pdf/1905.12588.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/khurramjaved96/mrcl" target="_blank" rel="noopener noreferrer">[Code]</a> <a href="https://sites.ualberta.ca/~kjaved/posters/mrcl.pdf" target="_blank" rel="noopener noreferrer">[Poster]</a></p></li><li><p><strong>Reconciling Meta-Learning and Continual Learning with Online Mixtures of Tasks.</strong> <em>Ghassen Jerfel, et al.</em> NIPS 2019. <a href="https://arxiv.org/pdf/1812.06080.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li><li><p><strong>Continuous Meta-Learning without Tasks.</strong> <em>James Harrison, et al.</em> arXiv 2019. <a href="https://arxiv.org/pdf/1912.08866.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/StanfordASL/moca" target="_blank" rel="noopener noreferrer">[Code]</a> <a href="https://openreview.net/forum?id=r1l1myStwr" target="_blank" rel="noopener noreferrer">[OpenReview]</a></p><blockquote><p>Integrate Bayesian online changepoint detection algorithm with existing meta-learning approaches to enable meta-learning in task-unsegmented settings.</p></blockquote></li><li><p><strong>Task Agnostic Continual Learning via Meta Learning.</strong> <em>Xu He, et al.</em> ICML 2020 LifelongML Workshop. <a href="https://openreview.net/pdf?id=AeIzVxdJgeb" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li><li><p><strong>La-MAML: Look-ahead Meta Learning for Continual Learning.</strong> <em>Gunshi Gupta, et al.</em> NIPS 2020. <a href="https://arxiv.org/pdf/2007.13904.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/montrealrobotics/La-MAML" target="_blank" rel="noopener noreferrer">[Code]</a></p></li></ul><h3 id="reinforcement-learning" tabindex="-1"><a class="header-anchor" href="#reinforcement-learning" aria-hidden="true">#</a> + Reinforcement Learning</h3><ul><li><p><strong>Reinforced Continual Learning.</strong> <em>Ju Xu and Zhanxing Zhu.</em> NIPS 2018. <a href="https://papers.nips.cc/paper/7369-reinforced-continual-learning.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/xujinfan/Reinforced-Continual-Learning" target="_blank" rel="noopener noreferrer">[Code]</a></p></li><li><p><strong>Experience Replay for Continual Learning.</strong> <em>David Rolnick, et al.</em> NIPS 2019. <a href="https://papers.nips.cc/paper/8327-experience-replay-for-continual-learning.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li></ul><h3 id="generative-modeling" tabindex="-1"><a class="header-anchor" href="#generative-modeling" aria-hidden="true">#</a> + Generative Modeling</h3><ul><li><p><strong>Lifelong GAN: Continual Learning for Conditional Image Generation.</strong> <em>Mengyao Zhai, et al.</em> ICCV 2019. <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhai_Lifelong_GAN_Continual_Learning_for_Conditional_Image_Generation_ICCV_2019_paper.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li><li><p><strong>Generative Models from the perspective of Continual Learning.</strong> <em>Timothée Lesort, et al.</em> IJCNN 2019. <a href="https://arxiv.org/pdf/1812.09111.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/TLESORT/Generative_Continual_Learning" target="_blank" rel="noopener noreferrer">[Code]</a></p></li></ul><h3 id="bayesian" tabindex="-1"><a class="header-anchor" href="#bayesian" aria-hidden="true">#</a> Bayesian</h3><ul><li><p><strong>Variational Continual Learning.</strong> <em>Cuong V. Nguyen, et al.</em> ICLR 2018. <a href="https://openreview.net/pdf?id=BkQqq0gRb" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/nvcuong/variational-continual-learning" target="_blank" rel="noopener noreferrer">[Code]</a> (VCL)</p></li><li><p><strong>Online Structured Laplace Approximations For Overcoming Catastrophic Forgetting.</strong> <em>Hippolyt Ritter, et al.</em> NIPS 2018. <a href="https://arxiv.org/pdf/1805.07810.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li><li><p><strong>Improving and Understanding Variational Continual Learning.</strong> <em>Siddharth Swaroop, et al.</em> NIPS 2018 Continual Learning Workshop. <a href="https://arxiv.org/pdf/1905.02099.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/nvcuong/variational-continual-learning/tree/master/improved_ddm" target="_blank" rel="noopener noreferrer">[Code]</a></p></li><li><p><strong>Task Agnostic Continual Learning Using Online Variational Bayes.</strong> <em>Chen Zeno, et al.</em> arXiv 2018. <a href="https://arxiv.org/pdf/1803.10123.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/igolan/bgd" target="_blank" rel="noopener noreferrer">[Code]</a></p></li><li><p><strong>Continual Unsupervised Representation Learning.</strong> <em>Dushyant Rao, et al.</em> NIPS 2019. <a href="https://papers.nips.cc/paper/8981-continual-unsupervised-representation-learning.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/deepmind/deepmind-research/tree/master/curl" target="_blank" rel="noopener noreferrer">[Code]</a></p><blockquote><p>Continual learning without task boundaries via dynamic expansion and generative replay (VAE).</p></blockquote></li><li><p><strong>Reconciling Meta-Learning and Continual Learning with Online Mixtures of Tasks.</strong> <em>Ghassen Jerfel, et al.</em> NIPS 2019. <a href="https://arxiv.org/pdf/1812.06080.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li><li><p><strong>Uncertainty-based Continual Learning with Adaptive Regularization.</strong> <em>Hongjoon Ahn, et al.</em> NIPS 2019. <a href="https://arxiv.org/pdf/1905.11614.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/csm9493/UCL" target="_blank" rel="noopener noreferrer">[Code]</a></p></li><li><p><strong>Continuous Meta-Learning without Tasks.</strong> <em>James Harrison, et al.</em> arXiv 2019. <a href="https://arxiv.org/pdf/1912.08866.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/StanfordASL/moca" target="_blank" rel="noopener noreferrer">[Code]</a> <a href="https://openreview.net/forum?id=r1l1myStwr" target="_blank" rel="noopener noreferrer">[OpenReview]</a></p><blockquote><p>Integrate Bayesian online changepoint detection algorithm with existing meta-learning approaches to enable meta-learning in task-unsegmented settings.</p></blockquote></li><li><p><strong>Task Agnostic Continual Learning via Meta Learning.</strong> <em>Xu He, et al.</em> ICML 2020 LifelongML Workshop. <a href="https://openreview.net/pdf?id=AeIzVxdJgeb" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li><li><p><strong>A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning.</strong> <em>Soochan Lee, et al.</em> ICLR 2020. <a href="https://openreview.net/pdf?id=SJxSOJStPr" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/soochan-lee/CN-DPM" target="_blank" rel="noopener noreferrer">[Code]</a></p></li><li><p><strong>Continual Learning with Bayesian Neural Networks for Non-Stationary Data.</strong> <em>Richard Kurle, et al.</em> ICLR 2020. <a href="https://openreview.net/pdf?id=SJlsFpVtDB" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li><li><p><strong>Uncertainty-guided Ccontinual Learning with Bayesian Neural Networks.</strong> <em>Sayna Ebrahimi, et al.</em> ICLR 2020. <a href="https://arxiv.org/pdf/1906.02425.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/SaynaEbrahimi/UCB" target="_blank" rel="noopener noreferrer">[Code]</a></p></li></ul><h3 id="new-settings" tabindex="-1"><a class="header-anchor" href="#new-settings" aria-hidden="true">#</a> New Settings</h3><ul><li><p><strong>Online Fast Adaptation and Knowledge Accumulation (OSAKA): a New Approach to Continual Learning.</strong> <em>Massimo Caccia, et al.</em> arXiv 2020. <a href="https://arxiv.org/pdf/2003.05856.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/ElementAI/osaka" target="_blank" rel="noopener noreferrer">[Code]</a></p></li><li><p><strong>Wandering Within a World: Online Contextualized Few-Shot Learning.</strong> <em>Mengye Ren, et al.</em> arXiv 2020. <a href="https://arxiv.org/pdf/2007.04546.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p></li><li><p><strong>Compositional Language Continual Learning.</strong> <em>Yuanpeng Li, et al.</em> ICLR 2020. <a href="https://openreview.net/pdf?id=rklnDgHtDS" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/yli1/CLCL" target="_blank" rel="noopener noreferrer">[Code]</a></p><blockquote><p>Continual learning in NLP for seq2seq style tasks.</p></blockquote></li></ul><h2 id="workshops" tabindex="-1"><a class="header-anchor" href="#workshops" aria-hidden="true">#</a> Workshops</h2><ul><li><a href="https://sites.google.com/site/cldlnips2016/submissions" target="_blank" rel="noopener noreferrer">Continual Learning and Deep Networks Workshop, NIPS 2016</a></li><li><a href="https://erodner.github.io/continuouslearningcvpr2017/" target="_blank" rel="noopener noreferrer">Continuous and Open-Set Learning Workshop, CVPR 2017</a></li><li><a href="https://rlabstraction2016.wixsite.com/icml-2017" target="_blank" rel="noopener noreferrer">First Lifelong Machine Learning Workshop, ICML 2017</a></li><li><a href="https://sites.google.com/view/llarla2018/home" target="_blank" rel="noopener noreferrer">Second Lifelong Machine Learning Workshop, ICML 2018</a></li><li><a href="https://sites.google.com/view/continual2018" target="_blank" rel="noopener noreferrer">Continual learning Workshop, NeurIPS 2018</a></li><li><a href="https://sites.google.com/view/llarla/home" target="_blank" rel="noopener noreferrer">Third Lifelong Machine Learning Workshop, RLDM 2019</a></li><li><a href="https://sites.google.com/view/cl-icml/organizers?authuser=0" target="_blank" rel="noopener noreferrer">Workshop on Continual Learning, ICML 2020</a></li><li><a href="https://lifelongml.github.io/" target="_blank" rel="noopener noreferrer">4th Lifelong Machine Learning Workshop, ICML 2020</a></li></ul><!--]--></div><footer class="page-meta"><div class="meta-item edit-link"><svg class="ov-icon" style="font-size:1.08em;" aria-hidden="true" width="17.28" height="17.28" viewbox="-43.52 -43.52 599.04 599.04" fill="currentColor"><path d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3 0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9 0l60.1 60.1c18.8 18.7 18.8 49.1 0 67.9zM284.2 99.8L21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3 0-17l-111-111c-4.8-4.7-12.4-4.7-17.1 0zM124.1 339.9c-5.5-5.5-5.5-14.3 0-19.8l154-154c5.5-5.5 14.3-5.5 19.8 0s5.5 14.3 0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8 0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"/></svg><a class="external-link meta-item-label" href="https://github.com/Renovamen/what-if/edit/master/notes/papers/dl/continual-learning.md" rel="noopener noreferrer" target="_blank" aria-label="Edit this page on GitHub"><!--[--><!--]--> Edit this page on GitHub <!--[--><!--]--></a></div><div class="meta-item last-updated"><span class="meta-item-label">Last Updated: </span><!----></div><!----></footer><nav class="page-nav"><p class="inner"><!----><span class="next"><a href="/papers/dl/meta-learning.html" class="" aria-label="Meta Learning / Few-Shot Learning"><!--[--><!--]--> Meta Learning / Few-Shot Learning <!--[--><!--]--></a></span></p></nav><!--[--><!--]--></main><!--]--></div><!----></div><!--]--></div>
    <script type="module" src="/assets/app.dd6d0129.js" defer></script>
  </body>
</html>
